{
    "_comment": " model parameters",
    "model": {
        "type_map":	        ["Ni", "O"],
        "descriptor" : {
            "type":		    "se_e2_a",
            "sel":		    [50, 50],
            "rcut_smth":	4.6,
            "rcut":		    5.6,
            "neuron":		[25, 50, 100],
            "resnet_dt":	false,
            "axis_neuron":	16,
            "seed":		    1,
            "_comment":		" that's all"
        },
        "fitting_net" : {
            "neuron":		[240, 240, 240],
            "resnet_dt":	true,
            "seed":		    1,
            "_comment":		" that's all"
        },
        "spin" : {
            "use_spin":     [true, false],
            "virtual_len":  [0.4],
            "spin_norm":    [1.2737],
            "_comment":		" that's all"
        },
        "_comment":	        " that's all"
    },

    "learning_rate" : {
        "type":		        "exp",
        "decay_steps":	    5000,
        "start_lr":	        3.00e-3,	
        "stop_lr":	        1.05e-7,
        "_comment":	        " that's all"
    },

    "loss" : {
        "type":		        "ener_spin",
        "start_pref_e":	    0.02,
        "limit_pref_e":	    10,
        "start_pref_fr":	1000,
        "limit_pref_fr":	1,
        "start_pref_fm":	5000,
        "limit_pref_fm":	5,
        "start_pref_v":	    0,
        "limit_pref_v":	    0,
        "_comment":	        " that's all"
    },

    "training" : {
        "training_data": {
            "systems":          ["../data/train/"],
            "batch_size":	    1,
            "_comment":		    "that's all"
        },
        "validation_data": {
            "systems":		    ["../data/validation/"],
            "batch_size":	    1,
            "numb_btch":	    10,
            "_comment":		    "that's all"
        },
        "numb_steps":	        1000,
        "seed":		            1,
        "disp_file":	        "lcurve.out",
        "disp_freq":	        100,
        "save_freq":        	1000,
        "_comment":	            "that's all"
    },    

    "_comment":		        "that's all"
}

